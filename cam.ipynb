{
   "cells": [
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "2d7782b1",
         "metadata": {},
         "outputs": [],
         "source": [
            "%matplotlib inline\n",
            "import matplotlib.pyplot as plt\n",
            "import torch\n",
            "from torch.utils.data import Dataset\n",
            "import torch.nn.functional as F\n",
            "import torchvision.transforms as transforms\n",
            "from pycocotools.coco import COCO\n",
            "from PIL import Image\n",
            "import pylab\n",
            "pylab.rcParams['figure.figsize'] = (8.0, 10.0)\n",
            "from torch.utils.data import DataLoader\n",
            "import random\n",
            "from tqdm import tqdm\n",
            "\n",
            "from cam_net import AlexNet_GAP\n",
            "from cam_utils import label_data, overlay_cam_with_centroid, ResizeAndPad, ImageListDataset"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "62fd42c9",
         "metadata": {},
         "outputs": [],
         "source": [
            "dataDir='data'\n",
            "dataType='val2017'\n",
            "annFile='{}/annotations/instances_{}.json'.format(dataDir,dataType)\n",
            "coco=COCO(annFile)\n",
            "all_data = label_data(coco)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "0efc360c",
         "metadata": {},
         "outputs": [],
         "source": [
            "# Cross Validation\n",
            "random.shuffle(all_data)\n",
            "split = int(0.8 * len(all_data))\n",
            "train_data, val_data = all_data[:split], all_data[split:]\n",
            "\n",
            "# Transform\n",
            "transform = transforms.Compose([\n",
            "    ResizeAndPad(),\n",
            "    transforms.ToTensor(),\n",
            "])\n",
            "\n",
            "# Datasets\n",
            "train_dataset = ImageListDataset(train_data, dataDir, transform)\n",
            "val_dataset = ImageListDataset(val_data, dataDir, transform)\n",
            "\n",
            "# DataLoaders\n",
            "train_loader = tqdm(DataLoader(train_dataset, batch_size=32, shuffle=True), desc=\"Training\", leave=False)\n",
            "val_loader = tqdm(DataLoader(val_dataset, batch_size=32), desc=\"Validating\", leave=False)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "d8463a3a",
         "metadata": {},
         "outputs": [],
         "source": [
            "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
            "model = AlexNet_GAP(num_classes=2).to(device)\n",
            "criterion = torch.nn.CrossEntropyLoss()\n",
            "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
            "train_losses = []\n",
            "val_losses = []\n",
            "\n",
            "num_epochs = 1\n",
            "for epoch in range(num_epochs):\n",
            "    model.train()\n",
            "    running_train_loss = 0.0\n",
            "    for images, labels in train_loader:\n",
            "        images, labels = images.to(device), labels.to(device)\n",
            "\n",
            "        outputs = model(images)\n",
            "        loss = criterion(outputs, labels)\n",
            "        train_losses.append(loss.item())\n",
            "\n",
            "        optimizer.zero_grad()\n",
            "        loss.backward()\n",
            "        optimizer.step()\n",
            "\n",
            "        running_train_loss += loss.item()\n",
            "        train_loader.set_postfix(loss=loss.item())\n",
            "\n",
            "    model.eval()\n",
            "    running_val_loss = 0.0\n",
            "    with torch.no_grad():\n",
            "        for images, labels in val_loader:\n",
            "            images, labels = images.to(device), labels.to(device)\n",
            "\n",
            "            outputs = model(images)\n",
            "            loss = criterion(outputs, labels)\n",
            "\n",
            "            val_losses.append(loss.item())\n",
            "            running_val_loss += loss.item()\n",
            "            \n",
            "            val_loader.set_postfix(loss=loss.item())\n",
            "    \n",
            "    avg_train_loss = running_train_loss / len(train_loader)\n",
            "    avg_val_loss = running_val_loss / len(val_loader)\n",
            "\n",
            "    print(f\"  Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f}\")\n",
            "\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "005b9b44",
         "metadata": {},
         "outputs": [],
         "source": [
            "\n",
            "plt.figure(figsize=(10, 5))\n",
            "plt.plot(train_losses, label=\"Training Loss\", color='blue', alpha=0.7)\n",
            "plt.plot(val_losses, label=\"Val Loss\", color='orange', alpha=0.7)\n",
            "plt.xlabel(\"Iteration\")\n",
            "plt.ylabel(\"Loss\")\n",
            "plt.title(\"Training Loss Over Time\")\n",
            "plt.legend()\n",
            "plt.grid(True)\n",
            "plt.tight_layout()\n",
            "plt.show()"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "4be381b0",
         "metadata": {},
         "outputs": [],
         "source": [
            "testimage = Image.open(f'{dataDir}/images/{all_data[random.randint(0, len(all_data) - 1)][0]}').convert(\"RGB\")\n",
            "output = model(transforms.ToTensor()(testimage).unsqueeze(0).to(device))\n",
            "\n",
            "# Get feature maps and class weights\n",
            "feature_maps = model.feature_maps.squeeze(0)  # [256, H, W]\n",
            "weights = model.classifier.weight  # [num_classes, 256]\n",
            "\n",
            "# Choose predicted class\n",
            "pred_class = torch.argmax(output, dim=1).item()\n",
            "class_weights = weights[pred_class]  # [256]\n",
            "\n",
            "# Compute CAM\n",
            "cam = torch.einsum(\"c,chw->hw\", class_weights, feature_maps)  # [H, W]\n",
            "cam = F.relu(cam)\n",
            "cam = (cam - cam.min()) / (cam.max() - cam.min() + 1e-5)\n",
            "overlay_cam_with_centroid(testimage, cam, (0,0))"
         ]
      }
   ],
   "metadata": {
      "kernelspec": {
         "display_name": "coco-env",
         "language": "python",
         "name": "python3"
      },
      "language_info": {
         "codemirror_mode": {
            "name": "ipython",
            "version": 3
         },
         "file_extension": ".py",
         "mimetype": "text/x-python",
         "name": "python",
         "nbconvert_exporter": "python",
         "pygments_lexer": "ipython3",
         "version": "3.10.16"
      }
   },
   "nbformat": 4,
   "nbformat_minor": 5
}
